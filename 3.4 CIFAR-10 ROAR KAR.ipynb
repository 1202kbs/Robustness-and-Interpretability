{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from attacks import GM\n",
    "from classifiers import CIFAR_CNN\n",
    "from deepexplain.tensorflow import DeepExplain\n",
    "from trainers import Trainer\n",
    "from utils import save, unpickle, preprocess, scale, params_maker\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "datadir = './CIFAR10_data/'\n",
    "batches = [datadir + batch for batch in os.listdir(datadir)]\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    batch = unpickle(batches[i])\n",
    "\n",
    "    if i == 0:\n",
    "        data = batch[b'data'].astype(np.float32)\n",
    "        cifar = np.transpose(np.reshape(data, [-1, 3, 32, 32]), [0, 2, 3, 1])\n",
    "        labels = batch[b'labels']\n",
    "    else:\n",
    "        data = batch[b'data'].astype(np.float32)\n",
    "        cifar = np.concatenate((cifar, np.transpose(np.reshape(data, [-1, 3, 32, 32]), [0, 2, 3, 1])), axis=0)\n",
    "        labels = np.concatenate((labels, batch[b'labels']), axis=0)\n",
    "\n",
    "scaled_cifar = cifar / 127.5 - 1.0\n",
    "\n",
    "test_batch = unpickle(batches[5])\n",
    "cifar_test = np.transpose(np.reshape(test_batch[b'data'], [-1, 3, 32, 32]), [0, 2, 3, 1])\n",
    "scaled_cifar_test = cifar_test / 127.5 - 1.0\n",
    "labels_test = np.array(test_batch[b'labels'])\n",
    "\n",
    "data_train = (scaled_cifar, labels)\n",
    "data_test = (scaled_cifar_test, labels_test)\n",
    "\n",
    "cifar_mean = np.mean(cifar, axis=(0, 1, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(logdir):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    DNN = CIFAR_CNN(logdir, activation=tf.nn.relu)\n",
    "    DNN.load(sess)\n",
    "\n",
    "    gm = GM(DNN, eps=0.4, step_size=2, n_steps=40, norm='2', loss_type='xent')\n",
    "\n",
    "    train_acc = DNN.evaluate(sess, data_train)\n",
    "    test_acc = DNN.evaluate(sess, data_test)\n",
    "    adv_acc = DNN.evaluate(sess, (gm.attack(sess, data_test, batch_size=500), data_test[1]))\n",
    "\n",
    "    sess.close()\n",
    "\n",
    "    res = collections.OrderedDict([('Train', train_acc), ('Test', test_acc), ('Adv', adv_acc)])\n",
    "\n",
    "    savefile = logdir.replace('tf_logs', 'results/accuracy')[:-1] + '.pickle'\n",
    "    savedir = '/'.join(savefile.split('/')[:-1])\n",
    "\n",
    "    if not os.path.exists(savedir):\n",
    "        os.makedirs(savedir)\n",
    "\n",
    "    save(res, savefile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROAR KAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_remove(images, percentile, keep=False):\n",
    "    images = np.copy(images)\n",
    "\n",
    "    mask = np.random.binomial(1, (100 - percentile) / 100, size=images.shape[:-1])\n",
    "\n",
    "    if keep:\n",
    "        images[mask == 1] = cifar_mean\n",
    "    else:\n",
    "        images[mask == 0] = cifar_mean\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def remove(images, attributions, percentile, keep=False, random=False):\n",
    "    '''\n",
    "    images       : tensor of shape [N,H,W,C]\n",
    "    attributions : tensor of shape [N,H,W]\n",
    "    percentile   : scalar between 0 and 100, inclusive\n",
    "    keep         : if true keep q percent; otherwise remove q percent\n",
    "    '''\n",
    "\n",
    "    images = np.copy(images)\n",
    "\n",
    "    thresholds = np.percentile(attributions, 100 - percentile, axis=(1, 2), keepdims=True)\n",
    "\n",
    "    if keep:\n",
    "        images[attributions < thresholds] = cifar_mean\n",
    "    else:\n",
    "        images[attributions > thresholds] = cifar_mean\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def occlude_dataset(DNN, de, attribution, loss, percentiles, test=False, keep=False, random=False, batch_size=1000,\n",
    "                    savedir='./'):\n",
    "    if test:\n",
    "        Xs = cifar_test\n",
    "        ys = labels_test\n",
    "    else:\n",
    "        Xs = cifar\n",
    "        ys = labels\n",
    "\n",
    "    total_batch = math.ceil(len(Xs) / batch_size)\n",
    "\n",
    "    if not random:\n",
    "\n",
    "        hmaps = []\n",
    "\n",
    "        for i in tqdm(range(total_batch)):\n",
    "\n",
    "            batch_xs = Xs[i * batch_size:(i + 1) * batch_size]\n",
    "            batch_xs_scaled = scale(batch_xs)\n",
    "\n",
    "            if 'edge' in attribution:\n",
    "                attrs = Canny(batch_xs)\n",
    "            else:\n",
    "                attrs = preprocess(de.explain(attribution, loss, DNN.X, batch_xs_scaled), 0, 100, use_abs=True)\n",
    "\n",
    "            # Add small random noise so np.percentile works correctly\n",
    "            attrs += np.random.uniform(low=0.0, high=1e-8, size=attrs.shape)\n",
    "\n",
    "            hmaps.append(attrs)\n",
    "\n",
    "        hmaps = np.concatenate(hmaps, axis=0)\n",
    "\n",
    "    for percentile in tqdm(percentiles):\n",
    "\n",
    "        dataset = []\n",
    "\n",
    "        for i in range(total_batch):\n",
    "\n",
    "            batch_xs, batch_ys = Xs[i * batch_size:(i + 1) * batch_size], ys[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "            if random:\n",
    "                occluded_images = random_remove(batch_xs, percentile, keep)\n",
    "            else:\n",
    "                batch_attrs = hmaps[i * batch_size:(i + 1) * batch_size]\n",
    "                occluded_images = remove(batch_xs, batch_attrs, percentile, keep)\n",
    "\n",
    "            dataset.append(scale(occluded_images))\n",
    "\n",
    "        save(np.concatenate(dataset, axis=0),\n",
    "             savedir + '{}_{}_{}.pickle'.format('test' if test else 'train', attribution, percentile))\n",
    "\n",
    "\n",
    "def eval_roar_kar(logdir, keep, train_only=False):\n",
    "    def get_savedir():\n",
    "\n",
    "        savedir = logdir.replace('tf_logs', 'KAR' if keep else 'ROAR')\n",
    "\n",
    "        if not os.path.exists(savedir):\n",
    "            os.makedirs(savedir)\n",
    "\n",
    "        return savedir\n",
    "\n",
    "    percentiles = [10, 30, 50, 70, 90]\n",
    "\n",
    "    attribution_methods = [\n",
    "        ('Random', 'zero'),\n",
    "        ('Saliency', 'saliency'),\n",
    "        ('Grad * Input', 'grad*input'),\n",
    "    ]\n",
    "\n",
    "    attribution_methods = collections.OrderedDict(attribution_methods)\n",
    "\n",
    "    if not train_only:\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        sess = tf.InteractiveSession()\n",
    "\n",
    "        with DeepExplain(session=sess, graph=sess.graph) as de:\n",
    "\n",
    "            DNN = CIFAR_CNN(logdir, activation=tf.nn.relu)\n",
    "            DNN.load(sess)\n",
    "\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                labels=tf.stop_gradient(tf.one_hot(DNN.yi, depth=DNN.n_classes)), logits=DNN.logits)\n",
    "\n",
    "            for k, v in attribution_methods.items():\n",
    "                batch_size = 2500\n",
    "                occlude_dataset(DNN, de, v, loss, percentiles, False, keep, k == 'Random', batch_size, get_savedir())\n",
    "                occlude_dataset(DNN, de, v, loss, percentiles, True, keep, k == 'Random', batch_size, get_savedir())\n",
    "\n",
    "        sess.close()\n",
    "\n",
    "    ress = collections.OrderedDict([(k, []) for k in attribution_methods.keys()])\n",
    "\n",
    "    for _ in range(3):\n",
    "\n",
    "        for k, v in attribution_methods.items():\n",
    "\n",
    "            res = []\n",
    "\n",
    "            for p in percentiles:\n",
    "                occdir = get_savedir() + '{}_{}_{}.pickle'.format('{}', v, p)\n",
    "                data_train = (unpickle(occdir.format('train')), labels)\n",
    "                data_test = (unpickle(occdir.format('test')), labels_test)\n",
    "\n",
    "                tf.reset_default_graph()\n",
    "\n",
    "                DNN = CIFAR_CNN('tf_logs/exp3/ROAR_KAR/', activation=tf.nn.relu)\n",
    "\n",
    "                sess = tf.InteractiveSession()\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "                trainer = Trainer(sess, DNN, data_train)\n",
    "                trainer.train(20, p_epochs=30)\n",
    "\n",
    "                acc = DNN.evaluate(sess, data_test)\n",
    "\n",
    "                print('{}{} | Accuracy : {:.5f}'.format(k, p, acc))\n",
    "\n",
    "                res.append(acc)\n",
    "\n",
    "                sess.close()\n",
    "\n",
    "            ress[k].append(res)\n",
    "\n",
    "    res_mean = {k: np.mean(v, axis=0) for k, v in ress.items()}\n",
    "\n",
    "    savefile = logdir.replace('tf_logs', 'results/{}'.format('kar' if keep else 'roar'))[:-1] + '.pickle'\n",
    "    savedir = '/'.join(savefile.split('/')[:-1])\n",
    "\n",
    "    print('Saving results at ' + savefile)\n",
    "\n",
    "    if not os.path.exists(savedir):\n",
    "        os.makedirs(savedir)\n",
    "\n",
    "    if not os.path.isfile(savefile):\n",
    "\n",
    "        save(res_mean, savefile)\n",
    "\n",
    "    else:\n",
    "\n",
    "        res_prev = unpickle(savefile)\n",
    "\n",
    "        for k, v in res_mean.items():\n",
    "            res_prev[k] = v\n",
    "\n",
    "        save(res_prev, savefile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logdir_maker(attack_params):\n",
    "    return 'tf_logs/exp3/adv/GM/{}/'.format('_'.join([str(v) for k, v in attack_params]))\n",
    "\n",
    "\n",
    "def run(attack_params=None, train_first=True, run_roar_kar=True, keep=False, rk_train_only=False):\n",
    "    if attack_params:\n",
    "        logdir = logdir_maker(attack_params)\n",
    "    else:\n",
    "        logdir = 'tf_logs/exp3/standard/'\n",
    "\n",
    "    print(logdir)\n",
    "\n",
    "    if train_first:\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        DNN = CIFAR_CNN(logdir, activation=tf.nn.relu, attack=GM, attack_params=dict(attack_params))\n",
    "\n",
    "        sess = tf.InteractiveSession()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        trainer = Trainer(sess, DNN, data_train)\n",
    "        trainer.train(n_epochs=20, p_epochs=5)\n",
    "\n",
    "        sess.close()\n",
    "\n",
    "        eval_accuracy(logdir)\n",
    "\n",
    "    if run_roar_kar: eval_roar_kar(logdir, keep, rk_train_only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(train_first=False, run_roar_kar=True, keep=False)\n",
    "run(train_first=False, run_roar_kar=True, keep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_paramss = params_maker(['eps', 'step_size', 'n_steps', 'norm', 'loss_type'],\n",
    "                              [[round(0.08 * i, 2) for i in range(1, 21)], [2], [40], ['2'], ['xent']])\n",
    "\n",
    "for attack_params in attack_paramss:\n",
    "    run(attack_params, train_first=False, run_roar_kar=True, keep=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_paramss = params_maker(['eps', 'step_size', 'n_steps', 'norm', 'loss_type'],\n",
    "                              [[round(0.08 * i, 2) for i in range(6, 21)], [2], [40], ['2'], ['cw']])\n",
    "\n",
    "for attack_params in attack_paramss:\n",
    "    run(attack_params, train_first=False, run_roar_kar=True, keep=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_paramss = params_maker(['eps', 'step_size', 'n_steps', 'norm', 'loss_type'],\n",
    "                              [[round(0.08 * i, 2) for i in range(1, 21)], [2], [40], ['2'], ['xent']])\n",
    "\n",
    "for attack_params in attack_paramss:\n",
    "    run(attack_params, train_first=False, run_roar_kar=True, keep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_paramss = params_maker(['eps', 'step_size', 'n_steps', 'norm', 'loss_type'],\n",
    "                              [[round(0.08 * i, 2) for i in range(1, 21)], [2], [40], ['2'], ['cw']])\n",
    "\n",
    "for attack_params in attack_paramss:\n",
    "    run(attack_params, train_first=False, run_roar_kar=True, keep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_paramss = params_maker(['eps', 'step_size', 'n_steps', 'norm', 'loss_type'],\n",
    "                              [[round(0.002 * i, 3) for i in range(1, 21)], [0.02], [40], ['inf'], ['xent']])\n",
    "\n",
    "for attack_params in attack_paramss:\n",
    "    run(attack_params, train_first=False, run_roar_kar=True, keep=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_paramss = params_maker(['eps', 'step_size', 'n_steps', 'norm', 'loss_type'],\n",
    "                              [[round(0.002 * i, 3) for i in range(8, 21)], [0.02], [40], ['inf'], ['cw']])\n",
    "\n",
    "for attack_params in attack_paramss:\n",
    "    run(attack_params, train_first=False, run_roar_kar=True, keep=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_paramss = params_maker(['eps', 'step_size', 'n_steps', 'norm', 'loss_type'],\n",
    "                              [[round(0.002 * i, 3) for i in range(1, 21)], [0.02], [40], ['inf'], ['xent']])\n",
    "\n",
    "for attack_params in attack_paramss:\n",
    "    run(attack_params, train_first=False, run_roar_kar=True, keep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_paramss = params_maker(['eps', 'step_size', 'n_steps', 'norm', 'loss_type'],\n",
    "                              [[round(0.002 * i, 3) for i in range(1, 21)], [0.02], [40], ['inf'], ['cw']])\n",
    "\n",
    "for attack_params in attack_paramss:\n",
    "    run(attack_params, train_first=False, run_roar_kar=True, keep=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
